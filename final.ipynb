{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project: Sentences similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Team members:\n",
    "\n",
    "* Khanh Duong TRAN.\n",
    "* Brandon NGUELEWI TOUTSAP."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Goal: Compare two sentences similarity by calculating Cosine similarity\n",
    "### Main ideas:\n",
    "\n",
    "#### 1. Vectorize the sentences.\n",
    "#### 2. Calculate Cosine similarity.\n",
    "#### 3. Compare the similarity.\n",
    "#### 4. Visualization between the 2 vectors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We will try to compare these two sentences:\n",
    "\n",
    " * \"The weather is hot today!\"\n",
    " * \"It is a hot day.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-29T22:08:22.525292Z",
     "start_time": "2023-07-29T22:08:22.415152700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorize sentences\n",
    "\n",
    "#### Step includes:\n",
    "##### 1. Process the sentences.\n",
    "### comment: something\n",
    "##### 2. Tokenize the sentences.\n",
    "##### 3. Convert into array."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Process the sentences:\n",
    "###### Step includes:\n",
    "\n",
    "###### 1. Remove non-word character such as punctuation, numbers, emojis, etc.\n",
    "###### 2. Remove redundant spaces.\n",
    "###### 3. Convert all to lower case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_sentence(sentence: str):\n",
    "    \"\"\" Preprocess the sentence before converting into array.\n",
    "\n",
    "    Argument:\n",
    "    sentence: The sentence we want to preprocess.\n",
    "\n",
    "    Output: Clean sentence.\n",
    "    \"\"\"\n",
    "    cleaned_sentence: str = \"\"\n",
    "    only_w: str = ''.join(char for char in sentence if char.isalpha() or char.isspace())\n",
    "    no_space: str = ' '.join(only_w.split())\n",
    "    to_lwer: str = no_space.lower()\n",
    "    cleaned_sentence = to_lwer\n",
    "\n",
    "    return cleaned_sentence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test the function\n",
    "assert process_sentence(\"I haVe a nAme20\") == \"i have a name\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Tokenize the sentences:\n",
    "###### Step includes:\n",
    "\n",
    "###### 1. Split the sentences into each word individually.\n",
    "###### 2. Lemmatize each word.\n",
    "###### 3. Create a vocabulary to vectorize."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Credit: ChatGPT\n",
    "def lemmatize_word(word):\n",
    "    \"\"\" Return the word into it most basic form.\n",
    "\n",
    "    Argument:\n",
    "    word: Word that needs to be reduced into most basic form.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return lemmatizer.lemmatize(word, tag_dict.get(tag, wordnet.NOUN))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_vocab(words_list):\n",
    "    \"\"\" Return a dictionary of vocabulary for a list of words\n",
    "\n",
    "    Argument:\n",
    "    sentence: List of words that we want to create a dictionary of vocabulary.\n",
    "    \"\"\"\n",
    "    lemmatized_list_of_word = [lemmatize_word(word) for word in words_list]\n",
    "\n",
    "    unique_word_set: set = set(sorted(lemmatized_list_of_word))\n",
    "    vocab: dict = {word: index for index, word in enumerate(unique_word_set)}\n",
    "\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab = create_vocab([\"apple\", \"banana\", \"apple\", \"orange\", \"grape\", \"banana\"])\n",
    "vocab_2 = create_vocab([\"studying\", \"watching\", \"was\"])\n",
    "\n",
    "assert 'apple' in vocab\n",
    "assert 'orange' in vocab\n",
    "assert 'pear' not in vocab\n",
    "\n",
    "assert \"study\" in vocab_2\n",
    "assert \"watch\" in vocab_2\n",
    "assert \"be\" in vocab_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Convert into array:\n",
    "###### Base on the created vocabulary to convert a sentence into an array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorize(words_list, vocabulary):\n",
    "    \"\"\" Return the vector of the sentence.\n",
    "\n",
    "    Argument:\n",
    "    words_list: list of word to make a vector.\n",
    "    vocabulary: a dictionary of vocabulary from the sentences.\n",
    "    \"\"\"\n",
    "    sentence_vector: np.ndarray = np.zeros(len(vocabulary))\n",
    "\n",
    "    for word in words_list:\n",
    "        if word in vocabulary:\n",
    "            sentence_vector[vocabulary[word]] += 1\n",
    "\n",
    "    return sentence_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert np.array_equal(vectorize([\"apple\", \"banana\", \"apple\", \"orange\", \"grape\", \"banana\"], {'apple': 0, 'banana': 1, 'grape': 2, 'orange': 3}), np.array([2, 2, 1, 1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Putting all together for vectorizing the sentences\n",
    "##### This section summarizes all the previous steps before proceeding to the Cosine similarity calculation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \"\"\" Preprocess the sentence and convert into an array of words.\n",
    "\n",
    "    Argument:\n",
    "    sentence: The sentence we want to preprocess.\n",
    "\n",
    "    Output: Array of words.\n",
    "    \"\"\"\n",
    "\n",
    "    #Remove non-word\n",
    "    cleaned_sentence: str = ''.join(char for char in sentence if char.isalpha() or char.isspace())\n",
    "\n",
    "    #Remove redundant whitespaces\n",
    "    cleaned_sentence = ' '.join(cleaned_sentence.split())\n",
    "\n",
    "    #Convert to lowercase\n",
    "    cleaned_sentence = cleaned_sentence.lower()\n",
    "\n",
    "    return cleaned_sentence.split()\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    \"\"\" Return the word into it most basic form.\n",
    "\n",
    "    Argument:\n",
    "    word: Word that needs to be reduced into most basic form.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tag = nltk.pos_tag([word])[0][1][0].lower()\n",
    "    tag_dict = {\"a\": wordnet.ADJ,\n",
    "                \"n\": wordnet.NOUN,\n",
    "                \"v\": wordnet.VERB,\n",
    "                \"r\": wordnet.ADV}\n",
    "    return lemmatizer.lemmatize(word, tag_dict.get(tag, wordnet.NOUN))\n",
    "\n",
    "def create_vocab(sentences):\n",
    "    \"\"\" Return a dictionary of vocabulary from two sentences\n",
    "\n",
    "    Argument:\n",
    "    sentences: List of sentences that we want to create a dictionary of vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words_arr: np.ndarray = process_sentence(sentence)\n",
    "        words += words_arr\n",
    "\n",
    "    lemmatized_list_of_word = [lemmatize_word(word) for word in words]\n",
    "\n",
    "    unique_word_set: set = set(sorted(lemmatized_list_of_word))\n",
    "    vocab: dict = {word: index for index, word in enumerate(sorted(unique_word_set))} #using sorted() to make sure each run is consistent.\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def vectorize(sentence, vocabulary):\n",
    "    \"\"\" Return the vector of the sentence.\n",
    "\n",
    "    Argument:\n",
    "    sentence: The sentence we want to vectorize.\n",
    "    vocabulary: A dictionary of vocabulary from the predefined sentences.\n",
    "    \"\"\"\n",
    "    sentence_vector: np.ndarray = np.zeros(len(vocabulary))\n",
    "\n",
    "    words_list: np.ndarray = process_sentence(sentence)\n",
    "    for word in words_list:\n",
    "        if word in vocabulary:\n",
    "            sentence_vector[vocabulary[word]] += 1\n",
    "\n",
    "    return sentence_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence_1 = \"Finally a sunny day!!\"\n",
    "sentence_2 = \"It's a sunny day with clear skies.\"\n",
    "sentences = np.array([sentence_1, sentence_2])\n",
    "\n",
    "vocabulary = create_vocab(sentences)\n",
    "print(vocabulary)\n",
    "\n",
    "print(vectorize(sentence_1, vocabulary))\n",
    "print(vectorize(sentence_2, vocabulary))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vector_1 = vectorize(sentence_1, vocabulary)\n",
    "vector_2 = vectorize(sentence_2, vocabulary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Cosine similarity\n",
    "\n",
    "### The formula is:\n",
    "\n",
    "$$\n",
    "\\cos(θ) =  \\frac{A \\cdot B}{\\|A\\| \\times \\|B\\|}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate the dot product of two vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_dot_product(vector1, vector2):\n",
    "    \"\"\" Return the dot product of two vectors.\n",
    "\n",
    "    Argument:\n",
    "    vector1 and vector2: two vectors.\n",
    "    \"\"\"\n",
    "    dot_product: float = 0\n",
    "\n",
    "    #Formula: dot = a1*b1 + a2*b2 ... + an*bn, with n is the size of the array.\n",
    "    dot_product = sum(vector1[i] * vector2[i] for i in range(len(vector1)))\n",
    "\n",
    "    return dot_product"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert calculate_dot_product(vector_1, vector_2) == np.dot(vector_1, vector_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate the norm of the vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_norm(vector):\n",
    "    \"\"\" Return the norm of a vector.\n",
    "\n",
    "    Argument:\n",
    "    vector: Vector we want to calculate the norm.\n",
    "    \"\"\"\n",
    "\n",
    "    #Formula: squared root of the sum squared of all elements in an array.\n",
    "    sum_ele: int = sum(vector[i] ** 2 for i in range(len(vector)))\n",
    "    return np.sqrt(sum_ele)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert calculate_norm(vector_2) == np.linalg.norm(vector_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finally, calculate the cosine similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\" Return the cosine similarity of two vectors.\n",
    "\n",
    "    Argument:\n",
    "    vector1 and vector2: vectors to calculate.\n",
    "    \"\"\"\n",
    "    dot: int = calculate_dot_product(vector1, vector2)\n",
    "    norm_multi: float = calculate_norm(vector1) * calculate_norm(vector2)\n",
    "\n",
    "    cosine: float = dot / norm_multi\n",
    "    cosine_percentage = cosine * 100\n",
    "    return cosine_percentage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similitude = cosine_similarity(vector_1, vector_2)\n",
    "similitude"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare\n",
    "\n",
    "### We define the threshold for the similarity is 0.5."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_2_vectors(vector_1, vector_2, similitude):\n",
    "    \"\"\" Visualize the two vectors onto a 2-D dimension space with an angle in between.\n",
    "\n",
    "    Argument:\n",
    "    vector_1 and vector_2: The two vectors we want to visualize.\n",
    "    similitude: The calculated Cosine similarity\n",
    "    \"\"\"\n",
    "    angle_rad = np.arccos(similitude/100)\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Plot Vector 1 as an arrow\n",
    "    plt.arrow(0, 0, vector_1[0], vector_1[1], color='blue', width=0.05, label='Vector 1', alpha=0.5)\n",
    "    # Plot Vector 2 as an arrow\n",
    "    plt.arrow(0, 0, vector_2[0], vector_2[1], color='orange', width=0.05, label='Vector 2', alpha=0.5)\n",
    "\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "\n",
    "    # Add the angle between vectors as text annotation\n",
    "    plt.text(0.1, 1.2, f'Angle: {angle_deg:.2f}°', fontsize=12)\n",
    "\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Vector Representation and Angle')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_2_vectors(vector_1, vector_2, similitude)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An interactive way that can take user input to compare\n",
    "\n",
    "#### We also made an interactive version so you can test this by yourself. Note that, since we coded this by hand, it would not be as robust as using libraries' tools.\n",
    "\n",
    "#### Simply convert the below code cell into code by using `Esc` + `y`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sen_1: str = input(\"Enter your first sentence: \")\n",
    "sen_2: str = input(\"Enter your second sentence: \")\n",
    "arr_sens: np.ndarray = np.array([sen_1, sen_2])\n",
    "\n",
    "print(sen_1)\n",
    "print(sen_2)\n",
    "\n",
    "user_input_vocab: dict = create_vocab(arr_sens)\n",
    "\n",
    "vec_1 = vectorize(sen_1, user_input_vocab)\n",
    "vec_2 = vectorize(sen_2, user_input_vocab)\n",
    "\n",
    "similarity: float = cosine_similarity(vec_1, vec_2)\n",
    "print(similarity)\n",
    "\n",
    "visualize_2_vectors(vec_1, vec_2, similarity)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An addition to the project\n",
    "\n",
    "### Now that we see how the cosine similarity works and how we can leverage it into determine whether a text is positive or negative.\n",
    "\n",
    "### This is a subset of Data Science domain called **Sentiment Analysis**, frequently used to help improve Brands/products reputations on the market."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Main idea:\n",
    "\n",
    "#### 1. Pre-defined a positive and negative set of vocabulary to vectorize an unseen review.\n",
    "\n",
    "#### 2. Calculate the cosine similarity between positive/negative and unseen review.\n",
    "\n",
    "#### 3. Compare the two cosine, the higher *score* means it is positive or negative."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Taken some random reviews for vocabulary creation\n",
    "positive_reviews = [\n",
    "    \"This product is amazing! It exceeded all my expectations.\",\n",
    "    \"I love this app! It's so easy to use and has all the features I need.\",\n",
    "    \"The service was excellent and the staff was very friendly.\",\n",
    "    \"The food at this restaurant is delicious and the atmosphere is great.\",\n",
    "    \"I had a wonderful experience with this company. Their customer support is top-notch.\",\n",
    "    \"I'm impressed with the quality of this product. It's worth every penny.\",\n",
    "    \"This book is a must-read! I couldn't put it down.\",\n",
    "    \"The hotel staff went above and beyond to make our stay comfortable and enjoyable.\",\n",
    "    \"I highly recommend this movie. It's entertaining and well-made.\",\n",
    "    \"The concert was fantastic! The artist gave an incredible performance.\",\n",
    "    \"This app is a lifesaver! It helped me stay organized and on track.\",\n",
    "    \"The customer service team was so helpful and responsive.\",\n",
    "    \"I'm so happy with my purchase. The product arrived quickly and in perfect condition.\",\n",
    "    \"The staff at this store was friendly and knowledgeable.\",\n",
    "    \"The hotel room was clean and spacious. I had a great night's sleep.\",\n",
    "    \"I can't say enough good things about this restaurant. The food was outstanding.\",\n",
    "    \"I am extremely satisfied with this service. It saved me so much time and effort.\",\n",
    "    \"The movie was heartwarming and had a great message.\",\n",
    "    \"I had a fantastic time at the amusement park. The rides were thrilling.\",\n",
    "    \"This product is a game-changer. It's made my life so much easier.\",\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This product is terrible. It broke after just a few days of use.\",\n",
    "    \"I'm very disappointed with this app. It keeps crashing and has a lot of bugs.\",\n",
    "    \"The service at this restaurant was awful. The staff was rude and unhelpful.\",\n",
    "    \"I had a horrible experience with this company. They didn't deliver what they promised.\",\n",
    "    \"The food at this place was terrible. It was cold and tasteless.\",\n",
    "    \"This book was a waste of time. The plot was confusing and the characters were uninteresting.\",\n",
    "    \"The hotel room was dirty and poorly maintained. I would not stay there again.\",\n",
    "    \"I regret watching this movie. It was boring and poorly acted.\",\n",
    "    \"The concert was a disaster. The sound quality was terrible and the artist seemed uninterested.\",\n",
    "    \"I wouldn't recommend this product to anyone. It's not worth the money.\",\n",
    "    \"This app is useless. It doesn't do what it's supposed to do.\",\n",
    "    \"The customer service was a nightmare. I was put on hold for hours.\",\n",
    "    \"I was really disappointed with the quality of the product. It fell apart after a few uses.\",\n",
    "    \"The food at this restaurant was overpriced and tasted bland.\",\n",
    "    \"The book was full of errors and typos. It was poorly edited.\",\n",
    "    \"The hotel was noisy and I couldn't sleep well.\",\n",
    "    \"The movie was a complete waste of time. The plot was predictable.\",\n",
    "    \"I had a terrible experience with the delivery service. My package was lost.\",\n",
    "    \"The amusement park was crowded and the lines were ridiculously long.\",\n",
    "    \"This product is a scam. It doesn't work as advertised.\",\n",
    "]\n",
    "\n",
    "# Create the review vocab to calculate and vectorize\n",
    "reviews: np.ndarray = positive_reviews + negative_reviews\n",
    "review_vocabulary: dict = create_vocab(reviews)\n",
    "\n",
    "# Vectorize the positive and negative reviews individually\n",
    "positive_vectors = [vectorize(review, review_vocabulary) for review in positive_reviews]\n",
    "negative_vectors = [vectorize(review, review_vocabulary) for review in negative_reviews]\n",
    "\n",
    "#Try different unseen review to test, taken from internet\n",
    "#new_review: str = \"I recently purchased this product and I have to say I'm impressed. It's exactly what I was looking for and it works perfectly. The customer service was also very helpful when I had a question. I highly recommend it!\"\n",
    "new_review: str = \"I was really disappointed with this product. It didn't work as expected and the customer service was unhelpful. I would not recommend it to others.\"\n",
    "\n",
    "# Vectorize the new review\n",
    "review_vector = vectorize(new_review, review_vocabulary)\n",
    "\n",
    "# Calculate cosine similarity for the new review with each set of positive and negative reviews\n",
    "cosine_positive = [cosine_similarity(review_vector, vector) for vector in positive_vectors]\n",
    "cosine_negative = [cosine_similarity(review_vector, vector) for vector in negative_vectors]\n",
    "\n",
    "# Determine sentiment based on cosine similarity\n",
    "print(max(cosine_positive))\n",
    "print(max(cosine_negative))\n",
    "\n",
    "if max(cosine_positive) > max(cosine_negative):\n",
    "    print(\"Positive review\")\n",
    "else:\n",
    "    print(\"Negative review\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
